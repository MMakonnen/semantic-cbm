{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIAL MVP ALL IN HERE -> spread into proper files and implement properly\n",
    "\n",
    "- currently only uses: LSTMConceptCorrector\n",
    "- realignment (i.e., intervention updates) happens only within a cluster, more specifically:\n",
    "  A custom “three-state mask” pipeline for concept realignment, with:\n",
    "  0 = open (the LSTM can update/realign this concept),\n",
    "  1 = permanently locked to ground truth (once an intervention happens -> replace by ground truth),\n",
    "  2 = temporarily locked for the current iteration (but reverts to 0 in the next iteration if not permanently locked -> for out of cluster concepts)\n",
    "- for now use synthetic data generation\n",
    "\n",
    "TODO:\n",
    "\n",
    "- get 2nd perspective on code\n",
    "- make sure everything moved to GPU\n",
    "- extend by other models for alignment (GRU and MLP and ...)\n",
    "- spread code properly over multiple files and do not implement in notebook\n",
    "- fit into whole pipeline, i.e. using real data and CBM model\n",
    "- correct all variable and par names and make them in line with proposal/final text\n",
    "\n",
    "MINOR NOTES:\n",
    "\n",
    "- ensure that all clusters and labels have atleast one observation (synthetic data context)\n",
    "\n",
    "GOOD TEXT CHUNKS FOR WRITTING (shorten and cut out certain parts):\n",
    "\n",
    "## Concept Clusters and Implicit Propagation\n",
    "\n",
    "### Concept Clusters\n",
    "\n",
    "In machine learning models, especially in complex domains like medical diagnosis, **concept clusters** are essential. These clusters consist of related or interdependent concepts that collectively represent a broader phenomenon. For example, in diagnosing respiratory infections, concepts such as \"fever,\" \"cough,\" and \"fatigue\" may form a cluster. Clustering helps manage inherent dependencies among concepts, ensuring that interventions on one concept naturally influence others within the same cluster, thereby maintaining consistency and coherence in the model's predictions.\n",
    "\n",
    "### Implicit Propagation via LSTM\n",
    "\n",
    "To capture dependencies within concept clusters, **Long Short-Term Memory (LSTM)** networks are utilized for their ability to model **sequential dependencies** and **temporal patterns**. When an intervention targets a single concept, the LSTM effectively models the intricate relationships within the cluster. During training, intervening on a concept like \"fever\" influences related concepts such as \"cough\" and \"fatigue,\" enabling the LSTM to **implicitly propagate** the intervention's effects across the entire cluster. This fosters a holistic and interconnected understanding of the underlying concepts.\n",
    "\n",
    "## Pipeline Mechanism for Implicit Propagation\n",
    "\n",
    "### Intervention on Individual Concepts\n",
    "\n",
    "Central to the pipeline is the **intervene function**, which selects and applies interventions on individual concepts based on an **uncertainty-based policy**. For each sample in a batch, the function identifies the most critical concept—determined by its uncertainty—and sets it to its ground truth value, marking it as **permanently locked** (`mask=1`). Additionally, all other concepts within the same cluster are **temporarily locked** (`mask=2`) to prevent unintended modifications during the current intervention round. This targeted intervention provides a clear ground truth signal for the model to correct its predictions. Consequently, the LSTM observes these corrections and learns to adjust related concepts within the same cluster **implicitly**, enhancing the model's ability to refine its predictions systematically.\n",
    "\n",
    "### LSTM Concept Corrector\n",
    "\n",
    "The **LSTMConceptCorrector** is responsible for realigning and correcting concept vectors based on interventions. It processes sequences of concept vectors, capturing the **temporal evolution** of corrections across multiple intervention steps. The model inputs a combination of **locked** (intervened) and **open** (modifiable) concepts. Locked concepts remain unchanged, while open concepts are adjusted based on the LSTM's learned patterns. Temporarily locked concepts are held steady during the current intervention round but are reset to open (`mask=0`) after realignment, allowing for future interventions if necessary. Through successive intervention and correction cycles, the LSTM learns the dependencies within concept clusters, effectively modeling how changes in one concept influence others within the same group. This dynamic adjustment ensures coherence and accuracy in the model's concept representations.\n",
    "\n",
    "### Sample Trajectory Function\n",
    "\n",
    "The **sample trajectory function** simulates the iterative process of interventions and realignments. It orchestrates a sequence of intervention steps, each followed by a realignment phase handled by the LSTMConceptCorrector. By conducting multiple rounds of interventions, the function enables the LSTM to progressively refine its concept predictions based on prior corrections. This iterative approach enhances the model's ability to correct individual concepts and reinforces the **implicit propagation** of interventions across related concepts within clusters. Consequently, the model develops more accurate and interdependent concept representations, closely aligning with the underlying structure of the concept clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST A FEW THINGS TO BE AWARE OF:\n",
    "\n",
    "# B - Batch size\n",
    "# T - Number of time steps\n",
    "# k - number of concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNTHETIC DATA GENERATION\n",
    "\n",
    "\n",
    "def generate_synthetic_data(k, n, J, m, seed):\n",
    "    \"\"\"\n",
    "    k: number of concepts\n",
    "    n: number of observations\n",
    "    J: number of target classes\n",
    "    m: number of concept clusters\n",
    "    seed: random seed\n",
    "\n",
    "    Returns:\n",
    "      predicted_concepts: float in [0,1], shape (n, k)\n",
    "      groundtruth_concepts: binary in {0,1}, shape (n, k)\n",
    "      cluster_assignments: dict {cluster_id: [concept_indices]}\n",
    "      labels: integer class label for each observation in [0..J-1], shape (n,)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # predicted concepts in [0,1]\n",
    "    predicted_concepts = torch.rand(n, k)\n",
    "\n",
    "    # ground truth concepts in {0,1}\n",
    "    groundtruth_concepts = (torch.rand(n, k) > 0.5).float()\n",
    "\n",
    "    # create random cluster assignment\n",
    "    cluster_assignments = {cid: [] for cid in range(m)}\n",
    "    for concept_idx in range(k):\n",
    "        assigned_cluster = torch.randint(low=0, high=m, size=(1,)).item()\n",
    "        cluster_assignments[assigned_cluster].append(concept_idx)\n",
    "\n",
    "    # randomly assign labels in {0,...,J-1}\n",
    "    labels = torch.randint(low=0, high=J, size=(n,))\n",
    "\n",
    "    return predicted_concepts, groundtruth_concepts, cluster_assignments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVENTION POLICY\n",
    "\n",
    "\n",
    "def ucp(concepts, already_intervened_concepts):\n",
    "    \"\"\"\n",
    "    Uncertainty-based Concept Picking (UCP) policy.\n",
    "\n",
    "    Args:\n",
    "        concepts (torch.Tensor): Current concept values, shape (B, k).\n",
    "        already_intervened_concepts (torch.Tensor): Mask indicating interventions, shape (B, k).\n",
    "                                                  1 => permanently locked, 0 => open, 2 => temporarily locked.\n",
    "\n",
    "    Returns:\n",
    "        importances (torch.Tensor): Importance scores for each concept, shape (B, k).\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    # Importance inversely proportional to distance from 0.5\n",
    "    importances = 1.0 / (torch.abs(concepts - 0.5) + eps)\n",
    "\n",
    "    # Exclude permanently and temporarily locked concepts by setting their importance to a large negative value\n",
    "    importances[(already_intervened_concepts == 1) | (already_intervened_concepts == 2)] = -1e10\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVENTION FUNCTION\n",
    "\n",
    "\n",
    "def intervene(concepts, estimated_concepts, concept_to_cluster, already_intervened_concepts, groundtruth_concepts, \n",
    "             intervention_policy=ucp):\n",
    "    \"\"\"\n",
    "    Applies the intervention policy to select and intervene on individual concepts.\n",
    "\n",
    "    Args:\n",
    "        concepts (torch.Tensor): Current concept values, shape (B, k).\n",
    "        estimated_concepts (torch.Tensor): Estimated concept predictions, shape (B, k).\n",
    "        concept_to_cluster (list): List mapping each concept to its cluster, length k.\n",
    "        already_intervened_concepts (torch.Tensor): Mask indicating interventions, shape (B, k).\n",
    "                                                  1 => permanently locked, 0 => open, 2 => temporarily locked.\n",
    "        groundtruth_concepts (torch.Tensor): Ground truth concept values, shape (B, k).\n",
    "        intervention_policy (function): Function to compute importances.\n",
    "\n",
    "    Returns:\n",
    "        concepts_new (torch.Tensor): Updated concept values after intervention, shape (B, k).\n",
    "        intervened_concepts_new (torch.Tensor): Updated mask, shape (B, k).\n",
    "    \"\"\"\n",
    "    # Compute importances using the chosen policy\n",
    "    importances = intervention_policy(concepts, already_intervened_concepts)\n",
    "    \n",
    "    B, k = concepts.shape\n",
    "\n",
    "    # Select the most important concept to intervene on for each sample\n",
    "    concepts_to_intervene = torch.argmax(importances, dim=1)  # shape: (B,)\n",
    "\n",
    "    # Replace selected concepts with ground truth values\n",
    "    concepts_new = concepts.clone()\n",
    "    concepts_new[range(B), concepts_to_intervene] = groundtruth_concepts[range(B), concepts_to_intervene]\n",
    "\n",
    "    # Update the intervention mask to permanently lock the intervened concepts\n",
    "    intervened_concepts_new = already_intervened_concepts.clone()\n",
    "    intervened_concepts_new[range(B), concepts_to_intervene] = 1\n",
    "\n",
    "    # Temporarily lock other concepts in the same cluster\n",
    "    for b in range(B):\n",
    "        selected_concept = concepts_to_intervene[b].item()\n",
    "        cluster_id = concept_to_cluster[selected_concept]\n",
    "        cluster_concepts = [c for c in range(k) if concept_to_cluster[c] == cluster_id and c != selected_concept]\n",
    "        if cluster_concepts:\n",
    "            intervened_concepts_new[b, cluster_concepts] = 2  # Temporarily locked\n",
    "\n",
    "    return concepts_new, intervened_concepts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMConceptCorrector WITH 3-STATE MASK\n",
    "\n",
    "\n",
    "class LSTMConceptCorrector(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based model that realigns concept vectors based on interventions.\n",
    "\n",
    "    Mask values:\n",
    "        0 => open (LSTM can adjust this concept)\n",
    "        1 => permanently locked to ground truth (once intervened) -> entered ground truth value\n",
    "        2 => temporarily locked (cannot be changed in the current round) -> for concepts outside of intervention cluster\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMConceptCorrector.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features (concepts).\n",
    "            hidden_size (int): Number of features in the hidden state of the LSTM.\n",
    "            num_layers (int): Number of recurrent layers in the LSTM.\n",
    "            output_size (int): Number of output features (concepts).\n",
    "        \"\"\"\n",
    "        super(LSTMConceptCorrector, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Define a fully connected layer to map LSTM outputs to concept space\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def prepare_initial_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Prepares the initial hidden and cell states for the LSTM.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of samples in the batch.\n",
    "            device (torch.device): Device to place the hidden states.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (h0, c0) initial hidden and cell states.\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, inputs, mask, estimated_concepts, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTMConceptCorrector.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Current concept values, shape (B, T, k).\n",
    "            mask (torch.Tensor): Mask indicating interventions, shape (B, T, k).\n",
    "                                 0 => open, 1 => permanently locked, 2 => temporarily locked.\n",
    "            estimated_concepts (torch.Tensor): Estimated concept predictions, shape (B, T, k).\n",
    "            hidden (tuple): Initial hidden and cell states for the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated concept values after realignment, shape (B, T, k).\n",
    "            tuple: Updated hidden and cell states.\n",
    "        \"\"\"\n",
    "        # Define which concepts are open, permanently locked, and temporarily locked\n",
    "        mask_open = (mask == 0).float()          # 1.0 where open, 0.0 otherwise\n",
    "        mask_perma_locked = (mask == 1).float()  # 1.0 where permanently locked\n",
    "        mask_temp_locked = (mask == 2).float()   # 1.0 where temporarily locked\n",
    "\n",
    "        # Create input for LSTM:\n",
    "        # - For permanently locked concepts (mask=1), use the current inputs (ground truth).\n",
    "        # - For temporarily locked (mask=2) and open concepts (mask=0), use the estimated predictions.\n",
    "        x = mask_perma_locked * inputs + (mask_temp_locked + mask_open) * estimated_concepts  # Shape: (B, T, k)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden = self.lstm(x, hidden)  # lstm_out: (B, T, hidden_size)\n",
    "\n",
    "        # Map LSTM outputs to concept space and apply sigmoid activation\n",
    "        corrected_raw = torch.sigmoid(self.fc(lstm_out))  # Shape: (B, T, k)\n",
    "\n",
    "        # Combine corrected concepts with locked and temporarily locked concepts:\n",
    "        # - Keep permanently locked concepts as-is\n",
    "        # - Keep temporarily locked concepts as estimated predictions\n",
    "        # - Update open concepts with the LSTM's corrections\n",
    "        output = mask_perma_locked * inputs + mask_temp_locked * estimated_concepts + mask_open * corrected_raw  # Shape: (B, T, k)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def forward_single_timestep(self, inputs, mask, estimated_concepts, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass for a single time step.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Current concept values, shape (B, k).\n",
    "            mask (torch.Tensor): Mask indicating interventions, shape (B, k).\n",
    "                                 0 => open, 1 => permanently locked, 2 => temporarily locked.\n",
    "            estimated_concepts (torch.Tensor): Estimated concept predictions, shape (B, k).\n",
    "            hidden (tuple): Initial hidden and cell states for the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated concept values after realignment, shape (B, k).\n",
    "            tuple: Updated hidden and cell states.\n",
    "        \"\"\"\n",
    "        # Add a time dimension of 1 to match the LSTM's expected input shape\n",
    "        inputs_ = inputs.unsqueeze(1)          # Shape: (B, 1, k)\n",
    "        mask_ = mask.unsqueeze(1)              # Shape: (B, 1, k)\n",
    "        est_ = estimated_concepts.unsqueeze(1) # Shape: (B, 1, k)\n",
    "\n",
    "        # Forward pass through the LSTMConceptCorrector\n",
    "        out, hidden = self.forward(inputs_, mask_, est_, hidden)  # out: (B, 1, k)\n",
    "\n",
    "        # Remove the time dimension\n",
    "        out = out.squeeze(1)  # Shape: (B, k)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE TRAJECTORY FUNCTION\n",
    "\n",
    "\n",
    "def sample_trajectory(model, predicted_concepts, groundtruth_concepts, \n",
    "                     concept_to_cluster, max_interventions=3, intervention_policy=ucp):\n",
    "    \"\"\"\n",
    "    Simulates multiple rounds of interventions and realignments.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The concept corrector model.\n",
    "        predicted_concepts (torch.Tensor): Predicted concepts, shape (B, k).\n",
    "        groundtruth_concepts (torch.Tensor): Ground truth concepts, shape (B, k).\n",
    "        concept_to_cluster (list): List mapping each concept to its cluster, length k.\n",
    "        max_interventions (int): Number of intervention steps to perform.\n",
    "        intervention_policy (function): Function to compute importances.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: Concept vectors at each intervention step, length (max_interventions + 1),\n",
    "                              each tensor of shape (B, k).\n",
    "    \"\"\"\n",
    "    device = predicted_concepts.device\n",
    "    B, k = predicted_concepts.shape\n",
    "\n",
    "    # Initialize masks: 1 => permanently locked, 0 => open, 2 => temporarily locked\n",
    "    already_intervened_concepts = torch.zeros(B, k).to(device)\n",
    "\n",
    "    # Clone predicted concepts to start\n",
    "    current_concepts = predicted_concepts.clone()\n",
    "\n",
    "    # Prepare initial hidden state\n",
    "    hidden = model.prepare_initial_hidden(B, device)\n",
    "\n",
    "    # Store all steps for analysis\n",
    "    all_steps = [current_concepts.clone()]\n",
    "\n",
    "    for step in range(max_interventions):\n",
    "        # Apply intervention\n",
    "        concepts_new, intervened_concepts_new = intervene(\n",
    "            concepts=current_concepts,\n",
    "            estimated_concepts=predicted_concepts,\n",
    "            concept_to_cluster=concept_to_cluster,\n",
    "            already_intervened_concepts=already_intervened_concepts,\n",
    "            groundtruth_concepts=groundtruth_concepts,\n",
    "            intervention_policy=intervention_policy\n",
    "        )\n",
    "\n",
    "        # Update concepts and masks\n",
    "        current_concepts = concepts_new\n",
    "        already_intervened_concepts = intervened_concepts_new\n",
    "\n",
    "        # Realign with the model\n",
    "        corrected_concepts, hidden = model.forward_single_timestep(\n",
    "            inputs=current_concepts,\n",
    "            mask=already_intervened_concepts.float(),  # Convert mask to float for the model\n",
    "            estimated_concepts=predicted_concepts,\n",
    "            hidden=hidden\n",
    "        )\n",
    "\n",
    "        # Update current concepts with corrected values\n",
    "        current_concepts = corrected_concepts\n",
    "\n",
    "        # Reset temporary locks (mask=2) back to open (mask=0)\n",
    "        already_intervened_concepts = torch.where(already_intervened_concepts == 2, \n",
    "                                                 torch.zeros_like(already_intervened_concepts), \n",
    "                                                 already_intervened_concepts)\n",
    "\n",
    "        # Store the step\n",
    "        all_steps.append(current_concepts.clone())\n",
    "\n",
    "    return all_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # hyperpar synthetic data\n",
    "    k = 6           # Number of concepts\n",
    "    n = 100         # Number of observations\n",
    "    J = 3           # Number of target classes\n",
    "    m = 2           # Number of concept clusters\n",
    "    seed = 42       # Random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic data\n",
    "    predicted_concepts, groundtruth_concepts, cluster_assignments, labels = generate_synthetic_data(\n",
    "        k=k,\n",
    "        n=n,\n",
    "        J=J,\n",
    "        m=m,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Print Cluster Assignments for Reference\n",
    "    print(\"=== Cluster Assignments ===\")\n",
    "    for cid, c_list in cluster_assignments.items():\n",
    "        print(f\"Cluster {cid}: {c_list}\")\n",
    "    print()\n",
    "\n",
    "    # Prepare concept_to_cluster list\n",
    "    concept_to_cluster = [0] * k  # Initialize list\n",
    "    for cid, c_list in cluster_assignments.items():\n",
    "        for c in c_list:\n",
    "            concept_to_cluster[c] = cid\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(predicted_concepts, groundtruth_concepts, labels)\n",
    "    batch_size = 16\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize the LSTMConceptCorrector model\n",
    "    hidden_size = 16\n",
    "    num_layers = 1\n",
    "    output_size = k  # Same as number of concepts\n",
    "    model = LSTMConceptCorrector(input_size=k, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "    device = torch.device(\"cpu\")  # Change to \"cuda\" if GPU is available\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Move data to device\n",
    "    predicted_concepts = predicted_concepts.to(device)\n",
    "    groundtruth_concepts = groundtruth_concepts.to(device)\n",
    "\n",
    "    # Define loss criterion and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training hyperparameters\n",
    "    epochs = 10\n",
    "    max_interventions = 3  # Number of intervention steps per batch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (pred_c, gt_c, lbls) in enumerate(dataloader):\n",
    "            # Move batch data to device\n",
    "            pred_c = pred_c.to(device)  # Shape: (B, k)\n",
    "            gt_c = gt_c.to(device)      # Shape: (B, k)\n",
    "            lbls = lbls.to(device)      # Shape: (B,)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform multiple intervention steps and realignments\n",
    "            all_steps = sample_trajectory(\n",
    "                model=model,\n",
    "                predicted_concepts=pred_c,\n",
    "                groundtruth_concepts=gt_c,\n",
    "                concept_to_cluster=concept_to_cluster,\n",
    "                max_interventions=max_interventions,\n",
    "                intervention_policy=ucp\n",
    "            )\n",
    "\n",
    "            # Use the final corrected concepts for loss computation\n",
    "            final_corrected_concepts = all_steps[-1]  # Shape: (B, k)\n",
    "\n",
    "            # Compute loss against ground truth concepts\n",
    "            loss = criterion(final_corrected_concepts, gt_c)\n",
    "\n",
    "            # Backpropagation and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    # (Optional) Inspect final cluster assignments and model predictions\n",
    "    print(\"\\n=== Final Cluster Assignments ===\")\n",
    "    for cid, c_list in cluster_assignments.items():\n",
    "        print(f\"Cluster {cid}: {c_list}\")\n",
    "\n",
    "    # (Optional) Print some example corrected concepts\n",
    "    print(\"\\n=== Example Corrected Concepts ===\")\n",
    "    example_steps = sample_trajectory(\n",
    "        model=model,\n",
    "        predicted_concepts=predicted_concepts[:5],      # Take first 5 samples\n",
    "        groundtruth_concepts=groundtruth_concepts[:5],\n",
    "        concept_to_cluster=concept_to_cluster,\n",
    "        max_interventions=max_interventions,\n",
    "        intervention_policy=ucp\n",
    "    )\n",
    "\n",
    "    for step_idx, cvec in enumerate(example_steps):\n",
    "        print(f\"Step {step_idx}:\")\n",
    "        print(cvec)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cluster Assignments ===\n",
      "Cluster 0: [0, 4]\n",
      "Cluster 1: [1, 2, 3, 5]\n",
      "\n",
      "Epoch [1/10] | Average Loss: 0.5128\n",
      "Epoch [2/10] | Average Loss: 0.5079\n",
      "Epoch [3/10] | Average Loss: 0.5263\n",
      "Epoch [4/10] | Average Loss: 0.5074\n",
      "Epoch [5/10] | Average Loss: 0.5125\n",
      "Epoch [6/10] | Average Loss: 0.5175\n",
      "Epoch [7/10] | Average Loss: 0.5127\n",
      "Epoch [8/10] | Average Loss: 0.5187\n",
      "Epoch [9/10] | Average Loss: 0.5218\n",
      "Epoch [10/10] | Average Loss: 0.5058\n",
      "\n",
      "Training complete!\n",
      "\n",
      "=== Final Cluster Assignments ===\n",
      "Cluster 0: [0, 4]\n",
      "Cluster 1: [1, 2, 3, 5]\n",
      "\n",
      "=== Example Corrected Concepts ===\n",
      "Step 0:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408, 0.1332, 0.9346, 0.5936],\n",
      "        [0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739],\n",
      "        [0.2666, 0.6274, 0.2696, 0.4414, 0.2969, 0.8317],\n",
      "        [0.1053, 0.2695, 0.3588, 0.1994, 0.5472, 0.0062]])\n",
      "\n",
      "Step 1:\n",
      "tensor([[0.4813, 0.9150, 0.3829, 0.9593, 0.4593, 0.0000],\n",
      "        [0.4818, 0.7936, 0.9408, 0.1332, 0.4586, 0.0000],\n",
      "        [0.4852, 1.0000, 0.7411, 0.4294, 0.4579, 0.5739],\n",
      "        [0.4777, 0.6274, 0.2696, 0.0000, 0.4524, 0.8317],\n",
      "        [0.1053, 0.4629, 0.4466, 0.5613, 0.0000, 0.5692]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "\n",
      "Step 2:\n",
      "tensor([[0.0000, 0.4711, 0.4403, 0.5627, 0.3904, 0.0000],\n",
      "        [0.0000, 0.4667, 0.4394, 0.5586, 0.9346, 0.0000],\n",
      "        [0.0000, 1.0000, 0.4425, 0.5609, 0.8854, 0.5853],\n",
      "        [0.0000, 0.4737, 0.4423, 0.0000, 0.2969, 0.5717],\n",
      "        [0.4723, 0.0000, 0.3588, 0.1994, 0.0000, 0.0062]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "\n",
      "Step 3:\n",
      "tensor([[0.0000, 1.0000, 0.3829, 0.9593, 0.4688, 0.6009],\n",
      "        [0.0000, 0.0000, 0.9408, 0.1332, 0.4751, 0.5936],\n",
      "        [0.0000, 0.5677, 1.0000, 0.4294, 0.4742, 0.5739],\n",
      "        [0.0000, 1.0000, 0.2696, 0.4414, 0.4557, 0.8317],\n",
      "        [0.0000, 0.0000, 0.4388, 0.5634, 0.5472, 0.5780]],\n",
      "       grad_fn=<CloneBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN MAIN FUNCTION\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbm-leakage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
